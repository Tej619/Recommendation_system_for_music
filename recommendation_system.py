# -*- coding: utf-8 -*-
"""Recommendation_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qX52fbt7bx877Jjx6gGQaLaRLlKjXidl

from google.colab import drive
drive.mount('/content/drive')

!pip install pyspark
!pip install gdown

"""Download Java"""

!apt-get install openjdk-11-jdk -y
!java -version

"""Setting path for java"""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]

"""Downloading Dataset from Kaggle."""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("devdope/900k-spotify")

print("Path to dataset files:", path)

"""# Cleaning Data


"""

import pandas as pd
import numpy as np
import os

print(path)
print(os.listdir(path))

file_path = os.path.join(path, 'spotify_dataset.csv') # get dataset from path

df = pd.read_csv(file_path)

df.head()

"""Checking summary statistics"""

df.describe()

"""Dataset of needed columns"""

# keep only columns needed for recommendation system

cols_needed = ["Genre", "song", "Artist(s)", "emotion", "Release Date", "Tempo"]

df = df[cols_needed].copy()

"""String cleanup"""

# basic cleaning of text columns

text_cols = ["Genre", "song", "Artist(s)", "emotion", "Release Date"]

# strip leading and trailing whitespace and make empty stings NaN

for col in text_cols:
  df[col] = (df[col]
             .astype(str) # ensure string
             .str.strip() # remove extra spaces
             .replace(r'^\s*$', np.nan, regex=True)) # change blank to NaN

"""Droppng missing values from needed fields"""

# drop rows missing values in key identity fields

key_required_fields = ["Genre", "song", "Artist(s)"]

df.dropna(subset=key_required_fields, inplace=True)

# drop rows missing values in supporting fields

df.dropna(subset=["emotion", "Release Date", "Tempo"], inplace=True)

"""Parse and clean the Release Date field"""

# convert Release Date into datetime data type
df["Release Date"] = pd.to_datetime(df["Release Date"], errors = "coerce")

# extract year for later use (e.g., old and recent songs)
df["release_year"] = df["Release Date"].dt.year

# Drop rows with invalid dates if they exist
df = df.dropna(subset=["Release Date", "release_year"])

# checking for missing values
df.isnull().sum()

"""Clean and Validate Tempo"""

# convert Tempo to numeric data type
df["Tempo"] = pd.to_numeric(df["Tempo"], errors="coerce")

# drop rows where tempo is missing or clearly invalid
df = df.dropna(subset=["Tempo"])

"""Check Tempo for outliers"""

# visualizing ditribution of Tempo values to spot outliers

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.hist(df["Tempo"], bins=50, edgecolor="black")
plt.title("Tempo Distribution")
plt.xlabel("Tempo (BPM)")
plt.ylabel("Count")
plt.show()

"""Dig deeper by looking at quartiles"""

# calculate the quantiles in inter-quartile range
q1 = df["Tempo"].quantile(0.25)
q3 = df["Tempo"].quantile(0.75)
iqr = q3 - q1

lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

print("Lower bound:", lower_bound)
print("Upper bound:", upper_bound)

outliers = df[(df["Tempo"] < lower_bound) | (df["Tempo"] > upper_bound)]
print("Outlier count:", len(outliers))

# outlier view
outliers.head()

# filter out extreme outliers (e.g., < 32 or > 205 BPM)
df = df[(df["Tempo"] >= 32) & (df["Tempo"] <= 205)]

"""Normalize text formats"""

# normalize Genre and emotion to lowercase
df["Genre"] = df["Genre"].str.lower()
df["emotion"] = df["emotion"].str.lower()

# renaming columns for easier use
df = df.rename(columns={
    "Genre": "genre",
    "song": "song_name",
    "Artist(s)": "artist_name",
    "Tempo" : "tempo",
    "Release Date" : "release_date"})

# cleaning numerice value for release_year
df["release_year"] = df["release_year"].astype("Int64")

df.head()

df.shape

"""Remove duplicate"""

# drop all duplicae rows
df = df.drop_duplicates()

# drop duplicate songs by artists
df = df.drop_duplicates(subset=["artist_name", "song_name"])

df.shape

"""Save clean dataset"""

df.to_csv("spotify_clean_dataset.csv", index = False)

print("Clean dataset size:", df.shape)

"""Feature Engineering

"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, trim, collect_set, array_join
from pyspark.sql import functions as F

spark = SparkSession.builder \
    .appName("GenreFeatureEngineering") \
    .getOrCreate()

"""Loading the data"""

df = spark.read.csv("spotify_clean_dataset.csv", header=True, inferSchema=True)

df.show(5, truncate=False)



"""Split genre into singular categories"""

df = df.withColumn("tempo", F.col("tempo").cast("int"))

df_genre_split = df.withColumn("genre", F.split(F.col("genre"), ",")) \
                   .withColumn("genre", F.explode(F.col("genre")))

df_genre_split.show(10, truncate=False)

# df_genre_split.write.mode("overwrite").csv("genre_split.csv", header=True)

# Compute min and max of tempo


tempo_stats = df_genre_split.agg(
    F.min("tempo").alias("min_tempo"),
    F.max("tempo").alias("max_tempo")
).collect()[0]

min_tempo = tempo_stats['min_tempo']
max_tempo = tempo_stats['max_tempo']

# Apply min-max normalization
df_normalized = df_genre_split.withColumn(
    "tempo_normalized",
    (F.col("tempo") - min_tempo) / (max_tempo - min_tempo)
)

df_normalized.show(10, truncate=False)

df_normalized.write.mode("overwrite").csv("featured_eng_normalized_tempo_songs.csv", header=True)



